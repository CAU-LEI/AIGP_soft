# aigp/auto_predictor.py
"""
è‡ªåŠ¨åŒ–åŸºå› ç»„é¢„æµ‹æ¨¡å—
æä¾›ä¸€é”®å¼æ¨¡å‹é€‰æ‹©ã€ä¼˜åŒ–å’Œæ¯”è¾ƒåŠŸèƒ½
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
import warnings
warnings.filterwarnings('ignore')

from .model_factory import get_model
from .trainer import run_ssa_search, run_grid_search
from .dim_reduction import reduce_dimensions


class AutoGenomicPredictor:
    """è‡ªåŠ¨åŒ–åŸºå› ç»„é¢„æµ‹å™¨"""
    
    def __init__(self, task_type="regression", cv=5, n_jobs=1, random_state=42):
        self.task_type = task_type
        self.cv = cv
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.results = {}
        
    def get_candidate_models(self):
        """è·å–å€™é€‰æ¨¡å‹åˆ—è¡¨"""
        if self.task_type == "regression":
            return {
                "LightGBM": {"class": "LGBMRegressor", "priority": 1},
                "CatBoost": {"class": "CatBoostRegressor", "priority": 1},
                "XGBoost": {"class": "xgboost", "priority": 2},
                "RandomForest": {"class": "RandomForest", "priority": 2},
                "GradientBoosting": {"class": "GradientBoosting", "priority": 3},
                "SVM": {"class": "svm", "priority": 3},
                "KNN": {"class": "knn", "priority": 4},
                "Ridge": {"class": "RidgeRegression", "priority": 4},
                "LinearRegression": {"class": "LinearRegression", "priority": 4},
                "ElasticNet": {"class": "ElasticNet", "priority": 4},
                "AdaBoost": {"class": "AdaBoost", "priority": 4}
            }
        else:  # classification
            return {
                "LightGBM": {"class": "LGBM", "priority": 1},
                "CatBoost": {"class": "CatBoost", "priority": 1},
                "XGBoost": {"class": "xgboost", "priority": 2},
                "RandomForest": {"class": "RandomForest", "priority": 2},
                "GradientBoosting": {"class": "GradientBoosting", "priority": 3},
                "SVM": {"class": "svm", "priority": 3},
                "KNN": {"class": "knn", "priority": 4},
                "LogisticRegression": {"class": "LogisticRegression", "priority": 4},
                "AdaBoost": {"class": "AdaBoost", "priority": 4},
                "ExtraTrees": {"class": "ExtraTrees", "priority": 4}
            }
    
    def get_preprocessing_options(self, n_features):
        """è·å–æ•°æ®é¢„å¤„ç†é€‰é¡¹"""
        options = {"none": None}
        
        # æ ¹æ®ç‰¹å¾æ•°é€‰æ‹©åˆé€‚çš„é™ç»´æ–¹æ³•
        if n_features > 1000:
            options["pca_100"] = {"method": "pca", "n_components": 100}
            options["pca_200"] = {"method": "pca", "n_components": 200}
        if n_features > 500:
            options["pca_50"] = {"method": "pca", "n_components": 50}
        if n_features > 100:
            options["phate_50"] = {"method": "phate", "n_components": 50}
            
        return options
    
    def evaluate_model(self, model, X, y, model_name):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        try:
            # äº¤å‰éªŒè¯è¯„ä¼°
            if self.task_type == "regression":
                # ä½¿ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°
                from sklearn.metrics import make_scorer
                from scipy.stats import pearsonr
                
                def pearson_corr(y_true, y_pred):
                    if len(y_true) < 2:
                        return 0
                    corr, _ = pearsonr(y_true, y_pred)
                    return corr
                
                scorer = make_scorer(pearson_corr, greater_is_better=True)
            else:
                scorer = 'accuracy'
                
            scores = cross_val_score(
                model, X, y, cv=self.cv, scoring=scorer, 
                n_jobs=self.n_jobs
            )
            
            if self.task_type == "regression":
                print(f"    âœ… {model_name} è¯„ä¼°æˆåŠŸ: çš®å°”é€Šç›¸å…³ç³»æ•° {scores.mean():.6f} Â± {scores.std():.6f}")
            else:
                print(f"    âœ… {model_name} è¯„ä¼°æˆåŠŸ: å‡†ç¡®ç‡ {scores.mean():.6f} Â± {scores.std():.6f}")
            return {
                'model_name': model_name,
                'cv_scores': scores,
                'cv_mean': scores.mean(),
                'cv_std': scores.std(),
                'model': model,
                'status': 'success'
            }
        except Exception as e:
            print(f"    âŒ {model_name} è¯„ä¼°å¤±è´¥: {e}")
            return {
                'model_name': model_name,
                'cv_scores': None,
                'cv_mean': -np.inf,
                'cv_std': 0,
                'model': model,
                'status': 'failed',
                'error': str(e)
            }
    
    def optimize_model(self, model, X, y, model_name):
        """ä¼˜åŒ–æ¨¡å‹è¶…å‚æ•°"""
        try:
            # ä¸ºä¸åŒæ¨¡å‹è®¾ç½®ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
            if model_name in ["LightGBM", "CatBoost"]:
                # ä½¿ç”¨SSAä¼˜åŒ–
                ssa_params = {
                    "use_custom_ssa": True,
                    "param_bounds": {
                        "learning_rate": [0.01, 0.3],
                        "num_leaves": [10, 100]
                    },
                    "pop_size": 10,
                    "max_iter": 20
                }
                optimized_model, score, params = run_ssa_search(
                    model, X, y, ssa_params, self.cv, self.task_type, self.n_jobs
                )
                return optimized_model, score, params
            else:
                # ä½¿ç”¨ç½‘æ ¼æœç´¢
                if model_name == "RandomForest":
                    grid_params = {
                        "n_estimators": [50, 100, 200],
                        "max_depth": [5, 10, 15]
                    }
                elif model_name == "SVM":
                    grid_params = {
                        "C": [0.1, 1, 10],
                        "gamma": ["scale", "auto"]
                    }
                else:
                    grid_params = {"n_estimators": [50, 100, 200]}
                
                optimized_model, score, params = run_grid_search(
                    model, X, y, grid_params, self.cv, self.task_type, self.n_jobs, self.random_state
                )
                return optimized_model, score, params
        except Exception as e:
            print(f"ä¼˜åŒ– {model_name} æ—¶å‡ºé”™: {e}")
            return model, -np.inf, {}
    
    def auto_predict(self, X, y, optimize=True, preprocess=True):
        """æ‰§è¡Œè‡ªåŠ¨åŒ–é¢„æµ‹"""
        print("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–åŸºå› ç»„é¢„æµ‹...")
        print(f"æ•°æ®å½¢çŠ¶: {X.shape}, ä»»åŠ¡ç±»å‹: {self.task_type}")
        
        # è·å–å€™é€‰æ¨¡å‹
        candidate_models = self.get_candidate_models()
        preprocessing_options = self.get_preprocessing_options(X.shape[1]) if preprocess else {"none": None}
        
        all_results = []
        
        # éå†é¢„å¤„ç†é€‰é¡¹
        for prep_name, prep_config in preprocessing_options.items():
            print(f"\nğŸ“Š æµ‹è¯•é¢„å¤„ç†: {prep_name}")
            
            # åº”ç”¨é¢„å¤„ç†
            if prep_config is None:
                X_processed = X
            else:
                X_processed = reduce_dimensions(
                    X, prep_config["method"], prep_config["n_components"]
                )
                print(f"é™ç»´åå½¢çŠ¶: {X_processed.shape}")
            
            # éå†æ¨¡å‹
            for model_name, model_info in candidate_models.items():
                print(f"  ğŸ” æµ‹è¯•æ¨¡å‹: {model_name}")
                
                try:
                    # åˆ›å»ºæ¨¡å‹
                    model = get_model(
                        self.task_type, model_info["class"], 
                        gpu=False, categorical=False
                    )
                    
                    # è¯„ä¼°åŸºç¡€æ¨¡å‹
                    base_result = self.evaluate_model(model, X_processed, y, model_name)
                    
                    if base_result['status'] == 'success':
                        result = {
                            'preprocessing': prep_name,
                            'model_name': model_name,
                            'base_cv_mean': base_result['cv_mean'],
                            'base_cv_std': base_result['cv_std'],
                            'optimized_cv_mean': base_result['cv_mean'],
                            'optimized_cv_std': base_result['cv_std'],
                            'model': base_result['model'],
                            'params': {},
                            'optimization': 'none'
                        }
                        
                        # ä¼˜åŒ–æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if optimize and model_name in ["LightGBM", "CatBoost", "RandomForest", "SVM"]:
                            print(f"    âš¡ ä¼˜åŒ– {model_name}...")
                            optimized_model, opt_score, opt_params = self.optimize_model(
                                model, X_processed, y, model_name
                            )
                            
                            if opt_score > base_result['cv_mean']:
                                result['model'] = optimized_model
                                result['optimized_cv_mean'] = opt_score
                                result['params'] = opt_params
                                result['optimization'] = 'ssa' if model_name in ["LightGBM", "CatBoost"] else 'grid'
                                print(f"    âœ… ä¼˜åŒ–æˆåŠŸ: {opt_score:.6f}")
                            else:
                                print(f"    âš ï¸  ä¼˜åŒ–æœªæ”¹å–„æ€§èƒ½")
                        
                        all_results.append(result)
                        print(f"    ğŸ“ˆ CVå¾—åˆ†: {result['optimized_cv_mean']:.6f} Â± {result['optimized_cv_std']:.6f}")
                    
                except Exception as e:
                    print(f"    âŒ {model_name} å¤±è´¥: {e}")
                    continue
        
        # æ’åºå¹¶é€‰æ‹©æœ€ä½³ç»“æœ
        all_results.sort(key=lambda x: x['optimized_cv_mean'], reverse=True)
        
        self.results = {
            'all_results': all_results,
            'best_result': all_results[0] if all_results else None,
            'task_type': self.task_type,
            'cv': self.cv,
            'n_samples': X.shape[0],
            'n_features': X.shape[1]
        }
        
        return self.results
    
    def print_summary(self):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        if not self.results or not self.results['all_results']:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„ç»“æœ")
            return
        
        print("\n" + "="*60)
        print("ğŸ¯ è‡ªåŠ¨åŒ–åŸºå› ç»„é¢„æµ‹ç»“æœæ‘˜è¦")
        print("="*60)
        
        best = self.results['best_result']
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best['model_name']}")
        print(f"ğŸ“Š é¢„å¤„ç†: {best['preprocessing']}")
        print(f"âš¡ ä¼˜åŒ–æ–¹æ³•: {best['optimization']}")
        if self.task_type == "regression":
            print(f"ğŸ“ˆ çš®å°”é€Šç›¸å…³ç³»æ•°: {best['optimized_cv_mean']:.6f} Â± {best['optimized_cv_std']:.6f}")
        else:
            print(f"ğŸ“ˆ å‡†ç¡®ç‡: {best['optimized_cv_mean']:.6f} Â± {best['optimized_cv_std']:.6f}")
        if best['params']:
            print(f"ğŸ”§ æœ€ä¼˜å‚æ•°: {best['params']}")
        
        print(f"\nğŸ“‹ æ‰€æœ‰æ¨¡å‹æ’å (å‰10å):")
        print("-" * 60)
        if self.task_type == "regression":
            print(f"{'æ’å':<4} {'æ¨¡å‹':<15} {'é¢„å¤„ç†':<12} {'çš®å°”é€Šç›¸å…³ç³»æ•°':<15} {'ä¼˜åŒ–':<8}")
        else:
            print(f"{'æ’å':<4} {'æ¨¡å‹':<15} {'é¢„å¤„ç†':<12} {'å‡†ç¡®ç‡':<12} {'ä¼˜åŒ–':<8}")
        print("-" * 60)
        
        for i, result in enumerate(self.results['all_results'][:10], 1):
            print(f"{i:<4} {result['model_name']:<15} {result['preprocessing']:<12} "
                  f"{result['optimized_cv_mean']:.6f} {result['optimization']:<8}")
    
    def save_detailed_results(self, output_file="detailed_results.txt"):
        """ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶"""
        if not self.results or not self.results['all_results']:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„ç»“æœ")
            return
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("AIGP è‡ªåŠ¨åŒ–åŸºå› ç»„é¢„æµ‹è¯¦ç»†ç»“æœ\n")
            f.write("="*50 + "\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            f.write(f"ä»»åŠ¡ç±»å‹: {self.results['task_type']}\n")
            f.write(f"äº¤å‰éªŒè¯æŠ˜æ•°: {self.results['cv']}\n")
            f.write(f"æ ·æœ¬æ•°: {self.results['n_samples']}\n")
            f.write(f"ç‰¹å¾æ•°: {self.results['n_features']}\n\n")
            
            # æœ€ä½³ç»“æœ
            if self.results.get('best_result'):
                best = self.results['best_result']
                f.write("æœ€ä½³æ¨¡å‹ç»“æœ:\n")
                f.write("-" * 30 + "\n")
                f.write(f"æ¨¡å‹åç§°: {best['model_name']}\n")
                f.write(f"é¢„å¤„ç†æ–¹æ³•: {best['preprocessing']}\n")
                f.write(f"ä¼˜åŒ–æ–¹æ³•: {best['optimization']}\n")
                if self.task_type == "regression":
                    f.write(f"çš®å°”é€Šç›¸å…³ç³»æ•°: {best['optimized_cv_mean']:.6f}\n")
                    f.write(f"æ ‡å‡†å·®: {best['optimized_cv_std']:.6f}\n")
                else:
                    f.write(f"å‡†ç¡®ç‡: {best['optimized_cv_mean']:.6f}\n")
                    f.write(f"æ ‡å‡†å·®: {best['optimized_cv_std']:.6f}\n")
                if best['params']:
                    f.write(f"æœ€ä¼˜å‚æ•°: {best['params']}\n")
                f.write("\n")
            
            # æ‰€æœ‰ç»“æœ
            f.write("æ‰€æœ‰æ¨¡å‹ç»“æœ:\n")
            f.write("-" * 30 + "\n")
            if self.task_type == "regression":
                f.write(f"{'æ’å':<4} {'æ¨¡å‹':<15} {'é¢„å¤„ç†':<12} {'çš®å°”é€Šç›¸å…³ç³»æ•°':<15} {'æ ‡å‡†å·®':<12} {'ä¼˜åŒ–':<8}\n")
                f.write("-" * 80 + "\n")
                
                for i, result in enumerate(self.results['all_results'], 1):
                    f.write(f"{i:<4} {result['model_name']:<15} {result['preprocessing']:<12} "
                           f"{result['optimized_cv_mean']:<15.6f} {result['optimized_cv_std']:<12.6f} "
                           f"{result['optimization']:<8}\n")
                
                f.write("\n")
                f.write("æ³¨æ„: ä½¿ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°è¯„ä¼°æ¨¡å‹æ€§èƒ½\n")
            else:
                f.write(f"{'æ’å':<4} {'æ¨¡å‹':<15} {'é¢„å¤„ç†':<12} {'å‡†ç¡®ç‡':<12} {'æ ‡å‡†å·®':<12} {'ä¼˜åŒ–':<8}\n")
                f.write("-" * 80 + "\n")
                
                for i, result in enumerate(self.results['all_results'], 1):
                    f.write(f"{i:<4} {result['model_name']:<15} {result['preprocessing']:<12} "
                           f"{result['optimized_cv_mean']:<12.6f} {result['optimized_cv_std']:<12.6f} "
                           f"{result['optimization']:<8}\n")
                
                f.write("\n")
                f.write("æ³¨æ„: ä½¿ç”¨å‡†ç¡®ç‡è¯„ä¼°æ¨¡å‹æ€§èƒ½\n")
        
        print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def get_best_model(self):
        """è·å–æœ€ä½³æ¨¡å‹"""
        if self.results and self.results['best_result']:
            return self.results['best_result']['model']
        return None
    
    def save_results(self, output_file="auto_predict_results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        import json
        
        if not self.results or not self.results.get('all_results'):
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„ç»“æœ")
            return
        
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„ç»“æœ
        serializable_results = {
            'task_type': self.results['task_type'],
            'cv': self.results['cv'],
            'n_samples': self.results['n_samples'],
            'n_features': self.results['n_features'],
            'best_result': None,
            'all_results': [
                {
                    'model_name': r['model_name'],
                    'preprocessing': r['preprocessing'],
                    'cv_mean': r['optimized_cv_mean'],
                    'cv_std': r['optimized_cv_std'],
                    'optimization': r['optimization']
                } for r in self.results['all_results']
            ]
        }
        
        # å¦‚æœæœ‰æœ€ä½³ç»“æœï¼Œæ·»åŠ æœ€ä½³ç»“æœä¿¡æ¯
        if self.results.get('best_result'):
            serializable_results['best_result'] = {
                'model_name': self.results['best_result']['model_name'],
                'preprocessing': self.results['best_result']['preprocessing'],
                'cv_mean': self.results['best_result']['optimized_cv_mean'],
                'cv_std': self.results['best_result']['optimized_cv_std'],
                'params': self.results['best_result']['params'],
                'optimization': self.results['best_result']['optimization']
            }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def auto_predict(geno_file, phe_file, task_type="regression", cv=5, n_jobs=4, 
                phe_col_num=3, category_cols=None, optimize=True, preprocess=True):
    """
    ä¸€é”®å¼è‡ªåŠ¨åŒ–åŸºå› ç»„é¢„æµ‹
    
    å‚æ•°:
        geno_file: åŸºå› å‹æ–‡ä»¶è·¯å¾„
        phe_file: è¡¨å‹æ–‡ä»¶è·¯å¾„
        task_type: ä»»åŠ¡ç±»å‹ ("regression" æˆ– "classification")
        cv: äº¤å‰éªŒè¯æŠ˜æ•°
        n_jobs: å¹¶è¡Œä½œä¸šæ•°
        phe_col_num: è¡¨å‹åˆ—å·
        category_cols: åå˜é‡åˆ—å·
        optimize: æ˜¯å¦è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        preprocess: æ˜¯å¦è¿›è¡Œæ•°æ®é¢„å¤„ç†
    """
    from .data_loader import load_training_data
    
    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    X, y, covariates = load_training_data(
        geno_file, '\s+', phe_file, '\s+', 
        phe_col_num, category_cols, task_type
    )
    
    # åˆ›å»ºè‡ªåŠ¨åŒ–é¢„æµ‹å™¨
    predictor = AutoGenomicPredictor(task_type=task_type, cv=cv, n_jobs=n_jobs)
    
    # æ‰§è¡Œè‡ªåŠ¨åŒ–é¢„æµ‹
    results = predictor.auto_predict(X, y, optimize=optimize, preprocess=preprocess)
    
    # æ‰“å°ç»“æœ
    predictor.print_summary()
    
    # ä¿å­˜ç»“æœ
    predictor.save_results()
    predictor.save_detailed_results()
    
    return predictor
